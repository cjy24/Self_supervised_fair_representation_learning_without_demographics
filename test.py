# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R55Yg51zo-6Ycn2VP9ghYrwfvPaHfkSN
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import torchvision
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.nn.modules.loss import _Loss
from torch import Tensor
from torchvision import datasets, models, transforms
from torchvision.datasets import ImageFolder

class Model(nn.Module):
    def __init__(self, feature_dim=128):
        super(Model, self).__init__()

        self.f = []
        for name, module in resnet50().named_children():
            if name == 'conv1':
                module = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False)
            if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):
                self.f.append(module)
        # encoder
        self.f = nn.Sequential(*self.f)
        # projection head
        self.g = nn.Sequential(nn.Linear(2048, 512, bias=False), nn.BatchNorm1d(512),
                               nn.ReLU(), nn.Linear(512, feature_dim, bias=True))

    def forward(self, x):
        x = self.f(x)
        feature = torch.flatten(x, start_dim=1)
        out = self.g(feature)
        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)

model.load_state_dict(torch.load('...'))
criterion = nn.CrossEntropyLoss()

model.eval()

def model_eval(actual, pred):

    confusion = pd.crosstab(actual, pred, rownames=['Actual'], colnames=['Predicted'])
    try:
      TP = confusion.loc[1,1] + 1
    except KeyError:
      TP = 1
    try:
      TN = confusion.loc[0,0] + 1
    except KeyError:
      TN = 1
    try:
      FP = confusion.loc[0,1] + 1
    except KeyError:
      FP = 1
    try:
      FN = confusion.loc[1,0] + 1
    except KeyError:
      FN = 1


    out = {}
    out['ALL'] = (TP+TN+FP+FN-4)
    out['DP'] = (TP+FP-2)/(TP+TN+FP+FN-4)
    out['TPR'] =  (TP-1)/(TP+FN-2)
    out['TNR'] = (TN-1)/(FP+TN-2)
    out['FPR'] = (FP-1)/(FP+TN-2)
    out['FNR'] = (FN-1)/(TP+FN-2)
    out['ACR'] = (TP+TN-2)/(TP+TN+FP+FN-4)

    return out

with torch.no_grad():
    running_loss = 0.
    running_corrects = 0
    epoch = 0

    for pos_1, pos_2, inputs, labels, sens, idx in train_bar:
        inputs = inputs
        labels = labels

        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        loss = criterion(outputs, labels)

        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)

    epoch_loss = running_loss / len(test_ds)
    epoch_acc = running_corrects / len(test_ds) * 100.
    idx_b = np.where(sens==1)
    y_b = labels[[idx_b]]
    pred_b = outputs[[idx_b]]
    pred_b = torch.squeeze(pred_b,0).data.max(1, keepdim=True)[1]
    idx_w = np.where(sens==0)
    y_w = labels[[idx_w]]
    pred_w = outputs[[idx_w]]
    pred_w = torch.squeeze(pred_w,0).data.max(1, keepdim=True)[1]
    print('[Test] Loss: {:.4f} Acc: {:.4f}%'.format(epoch_loss, epoch_acc))

w = model_eval(torch.squeeze(y_w,0), pred_w.detach().numpy().reshape(pred_w.shape[0]))
b = model_eval(torch.squeeze(y_b,0), pred_b.detach().numpy().reshape(pred_b.shape[0]))



DI1 = 100 * abs(w['DP'] - b['DP'])
DFPR1 = 100 * abs(w['TNR'] - b['TNR'])
DFNR1 = 100 * abs(w['TPR'] - b['TPR'])
w_TNR  = 100 * w['TNR']
w_TPR  = 100 * w['TPR']
b_TNR  = 100 * b['TNR']
b_TPR  = 100 * b['TPR']


print(f'Race: disparate impact is {DI1}%, eod is {DFPR1+DFNR1}%.')